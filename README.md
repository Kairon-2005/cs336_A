# CS336 Assignment 1 ‚Äî Transformer Language Model from Scratch

This repository contains my implementation of Stanford CS336 Assignment 1 (Spring 2025).
Following the assignment specification strictly, I implement a full Transformer language model from first principles, including tokenizer training, model architecture, optimization, training loop, and decoding.

All components are implemented using low-level PyTorch primitives only, in accordance with the ‚Äúfrom-scratch‚Äù requirement of the course.

## Project Structure 

cs336_basics/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ bpe.py          # Byte-level BPE tokenizer training
‚îú‚îÄ‚îÄ tokenizer.py    # Tokenizer encode / decode logic
‚îú‚îÄ‚îÄ model.py        # Transformer LM architecture
‚îú‚îÄ‚îÄ optimizer.py    # AdamW optimizer (from scratch)
‚îú‚îÄ‚îÄ utils.py        # Core utilities (loss, softmax, LR schedule, clipping)
‚îú‚îÄ‚îÄ train.py        # Training loop & checkpointing
‚îî‚îÄ‚îÄ decoding.py     # Autoregressive text generation

## Component Overview 

### 1. Byte-Level BPE Tokenizer (bpe.py, tokenizer.py)

Implements a byte-level Byte Pair Encoding (BPE) tokenizer, following Section 2 of the PDF.

Key features
	‚Ä¢	UTF-8 byte encoding (initial vocab size = 256)
	‚Ä¢	GPT-2 style regex pre-tokenization
	‚Ä¢	Deterministic BPE merge procedure
	‚Ä¢	No merges across pre-token or document boundaries
	‚Ä¢	Full support for special tokens (e.g. <|endoftext|>)
	‚Ä¢	Memory-efficient streaming tokenization (encode_iterable)

Responsibilities
	‚Ä¢	bpe.py: BPE training (vocab + merges)
	‚Ä¢	tokenizer.py: encoding / decoding using trained merges


### 2. Transformer Language Model (model.py)

Implements a decoder-only Transformer LM.

Architecture
	‚Ä¢	Token embedding
	‚Ä¢	Stack of pre-norm Transformer blocks
	‚Ä¢	Final RMSNorm
	‚Ä¢	Output projection (LM head)

Transformer block design
	‚Ä¢	RMSNorm ‚Üí Causal multi-head self-attention ‚Üí residual
	‚Ä¢	RMSNorm ‚Üí SwiGLU feed-forward network ‚Üí residual

Design choices 
	‚Ä¢	Pre-norm architecture
	‚Ä¢	RMSNorm instead of LayerNorm
	‚Ä¢	SwiGLU feed-forward (SiLU + GLU)
	‚Ä¢	Rotary Positional Embeddings (RoPE)
	‚Ä¢	No bias terms in linear layers
	‚Ä¢	Explicit causal masking

### 3. Optimization (optimizer.py, utils.py)

Implements all training utilities from scratch.

Loss
	‚Ä¢	Numerically stable cross-entropy loss
	‚Ä¢	Handles arbitrary batch dimensions

Optimizer
	‚Ä¢	Full AdamW implementation
	‚Ä¢	Correct moment tracking, bias correction, and decoupled weight decay
	‚Ä¢	Per-parameter optimizer state

Learning rate
	‚Ä¢	Cosine annealing schedule with warmup

Stability
	‚Ä¢	Gradient clipping by global $l_2$ norm

### 4. Training Loop (train.py)

Implements the full training pipeline described in Section 5 of the PDF.

Features
	‚Ä¢	Random subsequence sampling from a single token stream
	‚Ä¢	Memory-efficient dataset loading (numpy.memmap)
	‚Ä¢	Device-agnostic training (CPU / MPS / CUDA)
	‚Ä¢	Periodic validation evaluation
	‚Ä¢	Robust checkpoint save / resume
	‚Ä¢	Clean separation of model, optimizer, and scheduler state

### 5. Text Generation (decoding.py)

Implements autoregressive decoding as described in Section 6.

Supported features
	‚Ä¢	Temperature scaling
	‚Ä¢	Top-p (nucleus) sampling
	‚Ä¢	Early stopping on <|endoftext|>
	‚Ä¢	Configurable maximum generation length

This allows qualitative inspection of trained language models.

## Correctness & Testing
	‚Ä¢	All components are implemented to pass the official CS336 test suite
	‚Ä¢	Numerical stability and shape invariants are explicitly handled
	‚Ä¢	Adapter functions isolate test glue from core logic, as intended by the assignment


üìå Notes
	‚Ä¢	This repository follows Stanford CS336 academic guidelines.
	‚Ä¢	No high-level PyTorch abstractions (nn.Linear, nn.Embedding, torch.optim.Adam, etc.) are used.
	‚Ä¢	The implementation is suitable for small- to medium-scale experiments (TinyStories / OpenWebText).
